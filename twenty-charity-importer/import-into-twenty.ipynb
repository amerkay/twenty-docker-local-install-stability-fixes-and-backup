{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b79042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: dotenv in ./.venv/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: pyap in ./.venv/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests pandas dotenv openpyxl pyap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for rainrescue.co.uk, sleeping for 5 seconds...\n",
      "\n",
      "==================================================\n",
      "IMPORT SUMMARY\n",
      "==================================================\n",
      "Total rows processed: 244\n",
      "Companies created: 0\n",
      "Skipped (no domain): 18\n",
      "Skipped (already exists): 225\n",
      "Errors encountered: 0\n",
      "Success rate: 0.0%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyap\n",
    "from urllib.parse import urlparse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"TWENTY_KEY\")\n",
    "BASE_URL = os.getenv(\"TWENTY_BASE_URL\", \"http://twenty.local:3003\")\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "}\n",
    "\n",
    "\n",
    "class RateLimitException(Exception):\n",
    "    \"\"\"Exception raised when hitting rate limits.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def load_and_filter_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the Excel file and remove any rows where 'Contacted?' or\n",
    "    'Dogs / Cats?' are non-empty.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(filepath, engine=\"openpyxl\")\n",
    "\n",
    "    # Remove duplicate charities based on their unique Charity Number\n",
    "    df.drop_duplicates(subset=[\"Charity Number\"], inplace=True)\n",
    "\n",
    "    # Treat NaN as empty string for filtering\n",
    "    df = df.fillna({\"Contacted?\": \"\", \"Dogs / Cats?\": \"\"})\n",
    "    mask = (df[\"Contacted?\"] == \"\") & (df[\"Dogs / Cats?\"] == \"\")\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "\n",
    "def clean_domain(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract just the domain from a URL or hostname string,\n",
    "    stripping any scheme (http/https), 'www.' prefix, and paths.\n",
    "    \"\"\"\n",
    "    if not url or pd.isna(url):\n",
    "        return \"\"\n",
    "    # Ensure there's a scheme so urlparse will treat the input as a URL\n",
    "    if not re.match(r\"https?://\", url):\n",
    "        url = f\"https://{url}\"\n",
    "    parsed = urlparse(url)\n",
    "    # Strip 'www.' prefix if present (but only as a prefix)\n",
    "    domain = parsed.netloc\n",
    "    if domain.startswith(\"www.\"):\n",
    "        domain = domain[4:]  # Remove the first 4 characters \"www.\"\n",
    "    return domain\n",
    "\n",
    "\n",
    "def parse_income(value) -> int | None:\n",
    "    \"\"\"\n",
    "    Convert a numeric or string income into micros (int).\n",
    "    Returns None if the field is empty or invalid.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return None\n",
    "    # Strip out non-digit/decimal characters\n",
    "    if isinstance(value, str):\n",
    "        numeric = re.sub(r\"[^\\d.]\", \"\", value)\n",
    "        if not numeric:\n",
    "            return None\n",
    "        amount = float(numeric)\n",
    "    else:\n",
    "        amount = float(value)\n",
    "    return int(amount * 1_000_000)\n",
    "\n",
    "\n",
    "def company_exists(domain: str) -> bool:\n",
    "    \"\"\"\n",
    "    Query TwentyCRM for a company with the given domain using GraphQL.\n",
    "    Returns True if at least one match is found.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    query GetCompanies($filter: CompanyFilterInput) {\n",
    "        companies(filter: $filter) {\n",
    "            edges {\n",
    "                node {\n",
    "                    id\n",
    "                    domainName {\n",
    "                        primaryLinkUrl\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    variables = {\n",
    "        \"filter\": {\n",
    "            \"and\": [\n",
    "                {\"domainName\": {\"primaryLinkUrl\": {\"ilike\": f\"%{domain}\"}}},\n",
    "                {\n",
    "                    \"or\": [\n",
    "                        {\"deletedAt\": {\"is\": \"NULL\"}},\n",
    "                        {\"deletedAt\": {\"is\": \"NOT_NULL\"}},\n",
    "                    ]\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = {\"query\": query, \"variables\": variables}\n",
    "\n",
    "    resp = requests.post(f\"{BASE_URL}/graphql\", headers=HEADERS, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # Check for GraphQL errors\n",
    "    if \"errors\" in data:\n",
    "        # Check if any error contains \"Too many requests\"\n",
    "        for error in data[\"errors\"]:\n",
    "            if \"Too many requests\" in error.get(\"message\", \"\"):\n",
    "                raise RateLimitException(\"Too many requests\")\n",
    "        # For other GraphQL errors, raise an exception instead of returning False\n",
    "        raise Exception(f\"GraphQL errors: {data['errors']}\")\n",
    "\n",
    "    # Check if any companies were found\n",
    "    data_section = data.get(\"data\")\n",
    "    if not data_section:\n",
    "        raise Exception(\"No 'data' section in response\")\n",
    "\n",
    "    companies_section = data_section.get(\"companies\")\n",
    "    if not companies_section:\n",
    "        raise Exception(\"No 'companies' section in data\")\n",
    "\n",
    "    edges = companies_section.get(\"edges\", [])\n",
    "\n",
    "    return len(edges) > 0\n",
    "\n",
    "\n",
    "def create_company(payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Create a new company in TwentyCRM with the given payload.\n",
    "    \"\"\"\n",
    "    resp = requests.post(f\"{BASE_URL}/rest/companies\", headers=HEADERS, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def extract_address_fields(raw_address: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a raw UK address string and return structured fields\n",
    "    that map to TwentyCRM's address schema.\n",
    "    \"\"\"\n",
    "    # Try to parse; country='GB' for United Kingdom\n",
    "    addresses = pyap.parse(str(raw_address or \"\"), country=\"GB\")\n",
    "\n",
    "    if not addresses:\n",
    "        # Fallback: empty fields + default country\n",
    "        return {\n",
    "            \"addressStreet1\": \"\",\n",
    "            \"addressCity\": \"\",\n",
    "            \"addressState\": \"\",\n",
    "            \"addressPostcode\": \"\",\n",
    "            \"addressCountry\": \"United Kingdom\",\n",
    "        }\n",
    "\n",
    "    # Take the first match\n",
    "    addr_dict = addresses[0].as_dict()\n",
    "    # Build street line: prefer full_street, else number + name\n",
    "    street = addr_dict.get(\"full_street\") or \" \".join(\n",
    "        filter(None, [addr_dict.get(\"street_number\"), addr_dict.get(\"street_name\")])\n",
    "    )\n",
    "    return {\n",
    "        # \"addressStreet1\": street.strip(),\n",
    "        \"addressCity\": (addr_dict.get(\"city\") or \"\").strip(),\n",
    "        \"addressState\": (addr_dict.get(\"region1\") or \"\").strip(),\n",
    "        \"addressPostcode\": (addr_dict.get(\"postal_code\") or \"\").strip(),\n",
    "        \"addressCountry\": \"United Kingdom\",\n",
    "    }\n",
    "\n",
    "\n",
    "def build_payload(row: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Construct the JSON payload for a single row in the DataFrame,\n",
    "    using pyap to extract UK address components.\n",
    "    \"\"\"\n",
    "    domain = clean_domain(row[\"Website\"])\n",
    "\n",
    "    postCode = \"\"\n",
    "    charity_postcode = row.get(\"Charity Postcode\", \"\")\n",
    "    if (\n",
    "        charity_postcode\n",
    "        and not pd.isna(charity_postcode)\n",
    "        and str(charity_postcode).strip()\n",
    "    ):\n",
    "        postCode = str(charity_postcode).strip()\n",
    "    # Extract structured address fields\n",
    "    address_fields = extract_address_fields(\n",
    "        f\"{row.get(\"Charity Address\", \"\")}, {postCode}\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"name\": row[\"Charity Name\"].title(),\n",
    "        \"charityNumber\": str(row[\"Charity Number\"]),\n",
    "        \"domainName\": {\n",
    "            \"primaryLinkLabel\": \"\",\n",
    "            \"primaryLinkUrl\": domain,\n",
    "            \"additionalLinks\": [],\n",
    "        },\n",
    "        \"address\": address_fields,\n",
    "        \"whatTheyDo\": (\n",
    "            row.get(\"Activities\", \"\") or row.get(\"What the charity does\", \"\")\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Add ARR if available\n",
    "    micros = parse_income(row.get(\"Last Recorded Income\", \"\"))\n",
    "    if micros is not None:\n",
    "        payload[\"annualRecurringRevenue\"] = {\n",
    "            \"amountMicros\": micros,\n",
    "            \"currencyCode\": \"GBP\",\n",
    "        }\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "filepath = (\n",
    "    \"./full_uk_charitydetails_2025_07_08_11_28_38-\"\n",
    "    \"more-than-200000-exp-and-hospital sanctuary rescue rehab \"\n",
    "    \"rehabilitation wildlife marine.xlsx\"\n",
    ")\n",
    "df = load_and_filter_data(filepath)\n",
    "\n",
    "# Convert DataFrame to list of dictionaries to avoid iterator corruption\n",
    "rows_to_process = df.to_dict(\"records\")\n",
    "\n",
    "# Initialize counters for summary\n",
    "total_rows = len(rows_to_process)\n",
    "created_count = 0\n",
    "skipped_no_domain = 0\n",
    "skipped_exists = 0\n",
    "error_count = 0\n",
    "\n",
    "for row_dict in rows_to_process:\n",
    "    domain = clean_domain(row_dict[\"Website\"])\n",
    "    # print(f\"Processing {row_dict['Charity Name']} with domain: {domain}\")\n",
    "    if not domain:\n",
    "        # print(f\"Skipping '{row_dict['Charity Name']}' (no valid domain)\")\n",
    "        skipped_no_domain += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        if company_exists(domain):\n",
    "            # print(f\"Already exists: {domain}\")\n",
    "            skipped_exists += 1\n",
    "            continue\n",
    "\n",
    "        payload = build_payload(pd.Series(row_dict))\n",
    "        try:\n",
    "            result = create_company(payload)\n",
    "            print(\n",
    "                f\"Created: org for {domain}\"\n",
    "            )\n",
    "            created_count += 1\n",
    "        except requests.HTTPError as err:\n",
    "            print(f\"Error creating {domain}: {err}\")\n",
    "            error_count += 1\n",
    "\n",
    "        \n",
    "    except RateLimitException:\n",
    "        print(f\"Rate limit hit for {domain}, sleeping for 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        # Retry the same iteration by not incrementing the loop\n",
    "        # We need to decrement to retry this row\n",
    "        continue\n",
    "    except Exception as err:\n",
    "        print(f\"Error checking if company exists for {domain}: {err}\")\n",
    "        error_count += 1\n",
    "        continue\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IMPORT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total rows processed: {total_rows}\")\n",
    "print(f\"Companies created: {created_count}\")\n",
    "print(f\"Skipped (no domain): {skipped_no_domain}\")\n",
    "print(f\"Skipped (already exists): {skipped_exists}\")\n",
    "print(f\"Errors encountered: {error_count}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
